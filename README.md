# Representation of Word Meanings in Context

This is a reproduction and exploration of the [ACL '21 paper][1] titled "Exploring
the Representation of Word Meanings in Context: A Case Study on Homonymy and
Synonymy."  The study explores the capability of transformer-based models, e.g.,
BERT, to capture semantic meaning of words from context.

### Tasks

Enan

- [x] Get BERT embeddings for words in a sentence
- [ ] Dimension reduction of BERT embedding vectors
- [ ] Visualize dim-reduced vector

[1]: https://aclanthology.org/2021.acl-long.281/
